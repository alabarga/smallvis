% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/smallvis.R
\name{smallvis}
\alias{smallvis}
\title{Dimensionality Reduction via Neighbor Embedding}
\usage{
smallvis(X, k = 2, scale = "absmax", Y_init = "rand", perplexity = 30,
  inp_kernel = "gauss", max_iter = 1000, pca = FALSE, initial_dims = 50,
  method = "tsne", epoch_callback = TRUE,
  epoch = base::round(max_iter/10), min_cost = 0, momentum = 0.5,
  final_momentum = 0.8, mom_switch_iter = 250, eta = 500,
  min_gain = 0.01, exaggeration_factor = 1, stop_lying_iter = 100,
  gamma = 7, lveps = 0.1, spread = 1, min_dist = 0.001,
  ret_extra = FALSE, verbose = TRUE)
}
\arguments{
\item{X}{Input coordinates or distance matrix.}

\item{k}{Number of output dimensions for the embedding.}

\item{scale}{If \code{TRUE}, scale each column to zero mean and unit
variance. Alternatively, you may specify one of the following strings:
\code{"range"}, which range scales the matrix elements between 0 and 1;
\code{"absmax"}, here the columns are mean centered and then the elements
divided by absolute maximum value; \code{"scale"} does the same as using
\code{TRUE}. To use the input data as-is, use \code{FALSE}, \code{NULL}
or \code{"none"}.}

\item{Y_init}{How to initialize the output coordinates. See
the 'Output initialization' section.}

\item{perplexity}{The target perplexity for parameterizing the input
probabilities. For method \code{"umap"}, controls the neighborhood size
for parameterizing the smoothed k-nearest neighbor distances.}

\item{inp_kernel}{The input kernel function. Can be either \code{"gauss"}
(the default), or \code{"exp"}, which uses the unsquared distances.
\code{"exp"} is not the usual literature function, but matches the original
rtsne implementation (and it probably doesn't matter very much).}

\item{max_iter}{Maximum number of iterations in the optimization.}

\item{pca}{If \code{TRUE}, apply PCA to reduce the dimensionality of
\code{X} before any perplexity calibration, but after apply any scaling
and filtering. The number of principal components to keep is specified by
\code{initial_dims}. You may alternatively set this value to
\code{"whiten"}, in which case \code{X} is also whitened, i.e. the
principal components are scaled by the inverse of the square root of the
equivalent eigenvalues, so that the variance of component is 1.}

\item{initial_dims}{If carrying out PCA or whitening, the number of
principal components to keep. Must be no greater than the rank of the input
or no PCA or whitening will be carried out.}

\item{method}{A neighbor embedding method. See "Details".}

\item{epoch_callback}{Function to call after each epoch. See the
"Visualization callback" section. By default the current set of
coordinates will be plotted. Set to\code{FALSE} or \code{NULL} to turn
this off.}

\item{epoch}{After every \code{epoch} number of steps, calculates and
displays the cost value and calls \code{epoch_callback}, if supplied.}

\item{min_cost}{If the cost falls below this value, the optimization will
stop early.}

\item{momentum}{Initial momentum value.}

\item{final_momentum}{Final momentum value.}

\item{mom_switch_iter}{Iteration at which the momentum will switch from
\code{momentum} to \code{final_momentum}.}

\item{eta}{Learning rate value.}

\item{min_gain}{Minimum gradient descent step size.}

\item{exaggeration_factor}{Numerical value to multiply input probabilities
by, during the early exaggeration phase. May also provide the string
\code{"ls"}, in which case the dataset-dependent exaggeration technique
suggested by Linderman and Steinerberger (2017) is used. A value between
4-12 is normal. If using \code{Y_init = "laplacian"}, or supplying a matrix
of an existing configuration that you want refined, it is suggested not to
set this to \code{1} (effectively turning off early exaggeration).}

\item{stop_lying_iter}{Iteration at which early exaggeration is turned
off.}

\item{gamma}{Weighting term for the repulsive versus attractive forces in the
LargeVis and UMAP cost functions. Used only if \code{method = "largevis"}
or \code{"umap"}.}

\item{lveps}{Epsilon used in the LargeVis and UMAP gradient to prevent
division by zero. Used only if \code{method = "largevis"} or \code{"umap"}.
A comparatively large value (0.1) is recommended.}

\item{spread}{Parameter controlling the output kernel function for
\code{method = "umap"} only. Controls the length over which the output
kernel decays from 1 to 0.}

\item{min_dist}{Parameter controlling the output kernel function for
\code{method = "UMAP"} only. Controls the distance over which output
weights are clipped to 1.}

\item{ret_extra}{If \code{TRUE}, return value is a list containing additional
values associated with the embedding; otherwise just the output
coordinates. You may also provide a vector of names of potentially large or
expensive-to-calculate values to return, which will be returned in addition
to those value which are returned when this value is \code{TRUE}. See the
\code{Value} section for details.}

\item{verbose}{If \code{TRUE}, log progress messages to the console.}
}
\value{
If \code{ret_extra} is \code{FALSE}, the embedded output coordinates
  as a matrix. Otherwise, a list with the following items:
\itemize{
\item{\code{Y}} Matrix containing the embedded output coordinates.
\item{\code{N}} Number of objects.
\item{\code{origD}} Dimensionality of the input data.
\item{\code{scale}} Scaling applied to input data, as specified by the
  \code{scale} parameter.
\item{\code{Y_init}} Initialization type of the output coordinates, as
  specified by the \code{Y_init} parameter, or if a matrix was used, this will
  contain the string \code{"matrix"}.
\item{\code{iter}} Number of iterations the optimization carried out.
\item{\code{time_secs}} Time taken for the embedding, in seconds.
\item{\code{perplexity}} Target perplexity of the input probabilities, as
  specified by the \code{perplexity} parameter.
\item{\code{costs}} Embedding error associated with each observation. This is
  the sum of the absolute value of each component of the KL cost that the
  observation is associated with, so don't expect these to sum to the
  reported KL cost.
\item{\code{itercosts}} KL cost at each epoch.
\item{\code{stop_lying_iter}} Iteration at which early exaggeration is
  stopped, as specified by the \code{stop_lying_iter} parameter.
\item{\code{mom_switch_iter}} Iteration at which momentum used in
  optimization switches from \code{momentum} to \code{final_momentum}, as
  specified by the \code{mom_switch_iter} parameter.
\item{\code{momentum}} Momentum used in the initial part of the optimization,
  as specified by the \code{momentum} parameter.
\item{\code{final_momentum}} Momentum used in the second part of the
  optimization, as specified by the \code{final_momentum} parameter.
\item{\code{eta}} Learning rate, as specified by the \code{eta} parameter.
\item{\code{exaggeration_factor}} Multiplier of the input probabilities
  during the exaggeration phase. If the Linderman-Steinerberger exaggeration
  scheme is used, this value will have the name \code{"ls"}.
\item{\code{pca_dims}} If PCA was carried out to reduce the initial
  dimensionality of the input, the number of components retained, as
  specified by the \code{initial_dims} parameter.
\item{\code{whiten_dims}} If PCA whitening was carried out to reduce the
  dimensionality of the input, the number of components retained, as
  specified by the \code{initial_dims} parameter.
}
Additionally, if you set \code{ret_extra} to a vector of names, these will
be returned in addition to the values given above. These values are optional
and must be explicitly asked for, because they are either expensive to
calculate, take up a lot of memory, or both. The available optional values
are:
\itemize{
\item{\code{X}} The input data, after filtering and scaling.
\item{\code{P}} The input probabilities.
\item{\code{Q}} The output probabilities.
\item{\code{DX}} Input distance matrix. The same as \code{X} when the input
  data is already a distance matrix.
\item{\code{DY}} Output coordinate distance matrix.
}
}
\description{
Carry out dimensionality reduction of a (small) dataset using one of a
variety of neighbor embedding methods.
}
\details{
Currently supported embedding methods, which can be used as an argument
to the \code{method} parameter are:
\enumerate{
  \item \code{"tsne"} t-Distributed Stochastic Neighbor Embedding
  (van der Maaten and Hinton, 2008).
  \item \code{"largevis"} the cost function of the LargeVis algorithm
  (Tang et al, 2016).
  \item \code{"umap"} the cost function the UMAP method (McInnes, 2017).
  Unlike LargeVis and t-SNE, UMAP uses un-normalized input weights, which
  are calibrated via calculating smoothed k-nearest-neighbor distances,
  rather than perplexity (the procedure is similar, however).
}

Note that only the cost function is used from these methods in the context
of creating the full distance matrix as part of the gradient calculation.
None of the approximations or other speed-ups (e.g. Barnes-Hut or approximate
nearest neighbors routines) are used.
}
\section{Output initialization}{


For initializing the output coordinates, set the \code{Y_init} parameter
to one of the following:

\enumerate{
  \item{A matrix}: which must have dimensions \code{n} by \code{k}, where
  \code{n} is the number of rows in \code{X}.
  \item{\code{"rand"}}: initialize from a Gaussian distribution with mean 0
  and standard deviation 1e-4.
  \item{\code{"pca"}}: use the first \code{k} scores of the
  PCA: columns are centered, but no scaling beyond that which is applied by
  the \code{scale} parameter is carried out.
  \item{\code{"spca"}}: uses the PCA scores and then scales each score to a
  standard deviation of 1e-4.
  \item{\code{"laplacian"}}: initialize from Laplacian Eigenmap (Belkin and
  Niyogi, 2002).
}

As a spectral method, using \code{"laplacian"} is effectively the same as
turning off the repulsive interactions in the cost function (see Linderman
and Steinerberger, 2017): it is therefore unnecessary to use the
\code{exaggeration_factor} setting. However, it can be quite slow for larger
datasets. It should also behave similarly to the initialization method used
in UMAP.
}

\section{Visualization callback}{


During the optimization, the vizier package
(\url{https://www.github.com/jlmelville/vizier}) is used for visualization.
To use a custom callback, provide to the \code{epoch_callback} parameter a
function with the following signature:

\code{function(Y, iter, cost = NULL)}

where \code{Y} is the matrix of coordinates, \code{iter} is the current
iteration and \code{cost} is the current error value, which will be
\code{NULL} the first time this function is called (at iteration 0).
The function should have no return value, and presumably will call a plot
function. See the "Examples" section for the use of a custom callback.
Explicitly set \code{epoch_callback} to \code{NULL} or \code{FALSE} to turn
this off.
}

\examples{
\dontrun{

# tsne is the default. verbose = TRUE logs progress to console
# Also automatically uses github vizier package for plotting coordinates
# during optimization
tsne_iris <- smallvis(iris, perplexity = 50, verbose = TRUE)

# Can use a custom epoch_callback for visualization
colors = rainbow(length(unique(iris$Species)))
names(colors) = unique(iris$Species)
ecb = function(x, y) {
  plot(x, t = 'n')
  text(x, labels = iris$Species, col = colors[iris$Species])
}
tsne_iris <- smallvis(iris, epoch_callback = ecb, perplexity = 50, verbose = TRUE)

# To turn off visualization entirely:
tsne_iris <- smallvis(iris, epoch_callback = FALSE, perplexity = 50, verbose = TRUE)

# Try the LargeVis cost function, which also requires a gamma parameter to
# be specified:
largevis_iris <- smallvis(iris, method = "largevis", gamma = 7,
                          epoch_callback = ecb, perplexity = 50, verbose = TRUE)

# Use the UMAP cost function and input weights (perplexity here refers to the
# smoothed number of nearest neigbors)
umap_iris <- smallvis(iris, method = "umap", gamma = 1, eta = 0.1,
                      epoch_callback = ecb, perplexity = 50, verbose = TRUE)

# Use the early exaggeration suggested by Linderman and Steinerberger
tsne_iris_ls <- smallvis(iris, epoch_callback = ecb, perplexity = 50,
                         exaggeration_factor = "ls")

# Make embedding deterministic by initializing with scaled PCA scores
tsne_iris_spca <- smallvis(iris, epoch_callback = ecb, perplexity = 50,
                           exaggeration_factor = "ls", Y_init = "spca")

# Or use Laplacian Eigenmap for initialization (no exaggeration needed)
tsne_iris_lap <- smallvis(iris, epoch_callback = ecb, perplexity = 50,
                          Y_init = "laplacian")

# Return extra details about the embedding
tsne_iris_extra <- smallvis(iris, epoch_callback = ecb, perplexity = 50,
                            exaggeration_factor = "ls", Y_init = "spca", ret_extra = TRUE)

# Return even more details (which can be slow to calculate or take up a lot of memory)
tsne_iris_xextra <- smallvis(iris, epoch_callback = ecb, perplexity = 50,
                             exaggeration_factor = "ls", Y_init = "spca",
                             ret_extra = c("P", "Q", "X", "DX", "DY"))

# Reduce initial dimensionality to 3 via PCA
# (But you would normally do this with a much larger dataset)
tsne_iris_pca <- smallvis(iris, epoch_callback = ecb, perplexity = 50,
                          pca = TRUE, initial_dims = 3)

# Or use PCA whitening, so all columns of X have variance = 1
tsne_iris_whiten <- smallvis(iris, epoch_callback = ecb, perplexity = 50,
                             pca = "whiten", initial_dims = 3)
}
}
\references{
Belkin, M., & Niyogi, P. (2002).
Laplacian eigenmaps and spectral techniques for embedding and clustering.
In \emph{Advances in neural information processing systems}
(pp. 585-591).
\url{http://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering.pdf}

Van der Maaten, L., & Hinton, G. (2008).
Visualizing data using t-SNE.
\emph{Journal of Machine Learning Research}, \emph{9} (2579-2605).
\url{http://www.jmlr.org/papers/v9/vandermaaten08a.html}

Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April).
Visualizing large-scale and high-dimensional data.
In \emph{Proceedings of the 25th International Conference on World Wide Web}
(pp. 287-297).
International World Wide Web Conferences Steering Committee.
\url{https://arxiv.org/abs/1602.00370}

Linderman, G. C., & Steinerberger, S. (2017).
Clustering with t-SNE, provably.
\emph{arXiv preprint} \emph{arXiv}:1706.02582.
\url{https://arxiv.org/abs/1706.02582}

McInnes, L (2017).
UMAP: Universal Manifold Approximation and Mapping.
\url{https://github.com/lmcinnes/umap}
}
